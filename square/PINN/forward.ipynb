{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "baIfHKHMYTgq",
        "outputId": "579034ef-ba89-42f3-eb70-cc761f5de769"
      },
      "outputs": [],
      "source": [
        "# (for Google Colab)\n",
        "!pip install pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NVfVvdXPYah9"
      },
      "outputs": [],
      "source": [
        "import os, time, pickle\n",
        "import jax, flax, optax\n",
        "import jax.numpy as np\n",
        "import numpy as onp\n",
        "from functools import partial\n",
        "from pyDOE import lhs\n",
        "from typing import Sequence, Callable\n",
        "import json\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVvGhcu1YdSN",
        "outputId": "8d8aecd1-c9ce-4b02-8eb5-72aa7658afe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpu\n"
          ]
        }
      ],
      "source": [
        "# Must set CUDA_VISIBLE_DEVICES before importing JAX or any other library that initializes GPUs.\n",
        "# Otherwise, the environment variable change might be ignored.\n",
        "# \"0, 1\": first two GPUs / \"\": no GPU (CPU instead)\n",
        "\n",
        "# Run on the first GPU\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "from jax.extend.backend import get_backend\n",
        "print(get_backend().platform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9w3GnwiWTDm"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sWppminkWREX"
      },
      "outputs": [],
      "source": [
        "side_length = 40."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6Gt5NwYgpn"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Koe31PyMYe2K"
      },
      "outputs": [],
      "source": [
        "architecture_list = [[60, 60, 60, 60, 60, 2], [120, 120, 120, 120, 120, 2]] # NN architecture list\n",
        "# architecture_list = [[20, 2], [60, 2], [20, 20, 2], [60, 60, 2],\n",
        "#                      [20, 20, 20, 2], [60, 60, 60, 2],\n",
        "#                      [20, 20, 20, 20, 2], [60, 60, 60, 60, 2],\n",
        "#                      [20, 20, 20, 20, 20, 2], [60, 60, 60, 60, 60, 2],\n",
        "#                      [120, 120, 120, 120, 120, 2]] # NN architecture list\n",
        "lr = 5e-4 # learning rate\n",
        "num_epochs = 100000 # number of training epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_vI83g7YqDG"
      },
      "source": [
        "# NN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fZ7e8OwLYnIF"
      },
      "outputs": [],
      "source": [
        "# Define NN architecture\n",
        "class PDESolution(flax.linen.Module): # inherit from Module class\n",
        "    # One behavior of \"flax.linen.Module\" is to assign the provided argument to the \"self.features\"\n",
        "    features: Sequence[int] # dataclass (e.g. [10, 20, 2])\n",
        "\n",
        "    @flax.linen.compact # a decorator to define the model in more concise and readable way\n",
        "    def __call__(self, x): # __call__: makes an object callable, which enables you to use instances of the class like functions\n",
        "        for feature in self.features[:-1]:\n",
        "            x = flax.linen.tanh(flax.linen.Dense(feature)(x))\n",
        "        # Final Dense layer\n",
        "        x = flax.linen.Dense(self.features[-1])(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl49gaKNYuho"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvTHFz9NYr58"
      },
      "outputs": [],
      "source": [
        "# PDE residual\n",
        "@partial(jax.vmap, in_axes = (None, 0, 0), out_axes = 0)\n",
        "@partial(jax.jit, static_argnums = (0,)) # decorator closest to the function is applied first\n",
        "def residual(u, x, y):\n",
        "    def stress(x, y): # tensor\n",
        "        dim = 2\n",
        "        E = 2.35e3\n",
        "        nu = 0.33\n",
        "        mu = E / (2. * (1 + nu))\n",
        "        lmbda = E * nu / ((1 + nu) * (1 - 2*nu))\n",
        "        def u_vec(coords): # make (n, 2)\n",
        "            return u(coords[0], coords[1])\n",
        "        u_jac = jax.jacrev(u_vec) # (2, 2) Jacobian matrix\n",
        "        u_jac_val = u_jac((np.array([x, y])))\n",
        "        epsilon = 0.5 * (u_jac_val + u_jac_val.T) # (2, 2)\n",
        "        # Stress-Strain relationship\n",
        "        sigma = lmbda * np.trace(epsilon) * np.eye(dim) + 2 * mu * epsilon\n",
        "        return sigma\n",
        "\n",
        "    # σ = [[σ11, σ12], [σ21, σ22]]\n",
        "    jacobian_wrt_x = jax.jacobian(stress, argnums=0)(x, y) # [[∂σ11/∂x, ∂σ12/∂x], [∂σ21/∂x, ∂σ22/∂x]]\n",
        "    jacobian_wrt_y = jax.jacobian(stress, argnums=1)(x, y) # [[∂σ11/∂y, ∂σ12/∂y], [∂σ21/∂y, ∂σ22/∂y]]\n",
        "    dsigma11_dx = jacobian_wrt_x[0, 0] # ∂σ11 / ∂x\n",
        "    dsigma12_dy = jacobian_wrt_y[0, 1] # ∂σ12 / ∂y\n",
        "    dsigma21_dx = jacobian_wrt_x[1, 0] # ∂σ21 / ∂x\n",
        "    dsigma22_dy = jacobian_wrt_y[1, 1] # ∂σ22 / ∂y\n",
        "    lhs1 = dsigma11_dx + dsigma12_dy\n",
        "    rhs1 = 0.\n",
        "    lhs2 = dsigma21_dx + dsigma22_dy\n",
        "    rhs2 = 0.\n",
        "    return lhs1 - rhs1, lhs2 - rhs2\n",
        "\n",
        "# Neumann residual\n",
        "@partial(jax.vmap, in_axes=(None, 0, 0), out_axes=0)\n",
        "@jax.jit\n",
        "def traction(params, x, y):\n",
        "    x_cen, y_cen, a, b, gamma = np.array([25., 25., 10.0, 6.0, float(onp.pi/3)])\n",
        "    # Stress tensor\n",
        "    def stress(x, y):\n",
        "        dim = 2\n",
        "        E = 2.35e3\n",
        "        nu = 0.33\n",
        "        mu = E / (2. * (1 + nu))\n",
        "        lmbda = E * nu / ((1 + nu) * (1 - 2*nu))\n",
        "        def u_vec(coords): # make (n, 2)\n",
        "            return model.apply(params, coords.reshape(1, -1)).flatten()\n",
        "        u_jac = jax.jacrev(u_vec) # (2, 2) Jacobian matrix\n",
        "        u_jac_val = u_jac(np.array([x, y]))\n",
        "        epsilon = 0.5 * (u_jac_val + u_jac_val.T) # (2, 2)\n",
        "        # Stress-Strain relationship\n",
        "        sigma = lmbda * np.trace(epsilon) * np.eye(dim) + 2 * mu * epsilon\n",
        "        return sigma\n",
        "\n",
        "    # Normal vector\n",
        "    def ellipse_implicit(x, y, x_cen, y_cen, a, b, gamma):\n",
        "        lhs = (((x - x_cen) * np.cos(gamma) + (y - y_cen) * np.sin(gamma))**2 / a**2) +\\\n",
        "              (((x - x_cen) * np.sin(gamma) - (y - y_cen) * np.cos(gamma))**2 / b**2)\n",
        "        rhs = 1.\n",
        "        return lhs - rhs\n",
        "    ellipse_grad = jax.grad(ellipse_implicit, argnums=(0, 1))(x, y, x_cen, y_cen, a, b, gamma) # (n, 2)\n",
        "    ellipse_normal = -np.array(ellipse_grad) # (-) sign for the normals to point inward\n",
        "    ellipse_unit_normal = ellipse_normal / np.linalg.norm(ellipse_normal)\n",
        "\n",
        "    line_unit_normal1 = np.array([0., -1.]) # at line y = 0\n",
        "    line_unit_normal2 = np.array([0., 1.]) # at line y = side_length\n",
        "\n",
        "    # Traction\n",
        "    traction1 = np.dot(stress(x, 0.), line_unit_normal1) # (n, 2)\n",
        "    traction2 = np.dot(stress(x, side_length), line_unit_normal2)\n",
        "    traction3 = np.dot(stress(x, y), ellipse_unit_normal)\n",
        "    return traction1, traction2, traction3\n",
        "\n",
        "# Loss functionals\n",
        "@jax.jit\n",
        "def pde_residual1(params, points):\n",
        "    return np.mean(residual(lambda x, y: model.apply(params, np.stack((x, y))), points[:, 0], points[:, 1])[0] ** 2) # Mean Squared Error\n",
        "\n",
        "@jax.jit\n",
        "def pde_residual2(params, points):\n",
        "    return np.mean(residual(lambda x, y: model.apply(params, np.stack((x, y))), points[:, 0], points[:, 1])[1] ** 2) # Mean Squared Error\n",
        "\n",
        "@jax.jit\n",
        "def dirichlet_residual1(params, points): # Γ_u = -1 at x = 0\n",
        "    return np.mean((model.apply(params, np.stack((np.zeros_like(points[:,0]), points[:,1]), axis=1))[:, 0] + 10.) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def dirichlet_residual2(params, points): # Γ_u = 1 at x = 40\n",
        "    return np.mean((model.apply(params, np.stack((side_length * np.ones_like(points[:,0]), points[:,1]), axis=1))[:, 0] - 10.) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def dirichlet_residual3(params, points): # Γ_u = -1 at x = 0\n",
        "    return np.mean((model.apply(params, np.stack((np.zeros_like(points[:,0]), points[:,1]), axis=1))[:, 1]) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def dirichlet_residual4(params, points): # Γ_u = -1 at x = 0\n",
        "    return np.mean((model.apply(params, np.stack((side_length * np.ones_like(points[:,0]), points[:,1]), axis=1))[:, 1]) ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def neumann_residual1(params, points):\n",
        "    return np.mean(traction(params, points[:, 0], points[:, 1])[0] ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def neumann_residual2(params, points):\n",
        "    return np.mean(traction(params, points[:, 0], points[:, 1])[1] ** 2)\n",
        "\n",
        "@jax.jit\n",
        "def neumann_residual3(params, points):\n",
        "    return np.mean(traction(params, points[:, 0], points[:, 1])[2] ** 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qetyn9CgYxkg"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CoThK9pYvcZ"
      },
      "outputs": [],
      "source": [
        "# Total loss functional\n",
        "def loss_total(params, boundary_samples, angle_samples1):\n",
        "    # Some variables\n",
        "    x_cen, y_cen, a, b, gamma = np.array([25., 25., 10.0, 6.0, float(onp.pi/3)])\n",
        "    num_points = 80\n",
        "\n",
        "    # Define the domain & external boundary points\n",
        "    lb = np.array([0., 0.]) # lower bound\n",
        "    ub = np.array([side_length, side_length]) # upper bound\n",
        "    boundary_points = lb + (ub - lb) * boundary_samples\n",
        "\n",
        "    # Define the boundary points of the void\n",
        "    def ellipse(x_cen, y_cen, a, b, gamma, rotated_ang):\n",
        "        x = a * np.cos(rotated_ang) * np.cos(gamma) - b * np.sin(rotated_ang) * np.sin(gamma) + x_cen\n",
        "        y = a * np.cos(rotated_ang) * np.sin(gamma) + b * np.sin(rotated_ang) * np.cos(gamma) + y_cen\n",
        "        return np.column_stack((x, y))\n",
        "    random_angle1 = 2 * np.pi * angle_samples1\n",
        "    void_boundary_points = ellipse(x_cen, y_cen, a, b, gamma, random_angle1)\n",
        "\n",
        "    # Define the domain points\n",
        "    # Find the vertices of the domain\n",
        "    vertices = np.array([[0, 0],\n",
        "                            [side_length, 0],\n",
        "                            [side_length, side_length],\n",
        "                            [0, side_length]])\n",
        "    vertices_x, vertices_y = vertices[:, 0], vertices[:, 1]\n",
        "    # Find the intersection points of ellipse perimeter & the line between the center of ellipse and four vertices\n",
        "    ellipse_lhs = (((vertices_x - x_cen) * np.cos(gamma) + (vertices_y - y_cen) * np.sin(gamma))**2 / a**2) +\\\n",
        "        (((vertices_x - x_cen) * np.sin(gamma) - (vertices_y - y_cen) * np.cos(gamma))**2 / b**2)\n",
        "    t = 1 - 1 / np.sqrt(ellipse_lhs)\n",
        "    x_intersect = (1 - t) * vertices_x + t * x_cen\n",
        "    y_intersect = (1 - t) * vertices_y + t * y_cen\n",
        "    intersections = np.stack([x_intersect, y_intersect], axis=1)\n",
        "    # Untranslate & unrotate\n",
        "    x_intersect_untrans = x_intersect - x_cen\n",
        "    y_intersect_untrans = y_intersect - y_cen\n",
        "    x_intersect_unrotat = x_intersect_untrans * np.cos(gamma) + y_intersect_untrans * np.sin(gamma)\n",
        "    y_intersect_unrotat = -x_intersect_untrans * np.sin(gamma) + y_intersect_untrans * np.cos(gamma)\n",
        "    # Find the angles of intersection points\n",
        "    angles_intersect = np.arctan2(y_intersect_unrotat / b, x_intersect_unrotat / a)\n",
        "    angles_interpolate1 = np.linspace(angles_intersect[1], angles_intersect[2], num_points + 2)[1:-1]\n",
        "    angles_interpolate2 = np.linspace(angles_intersect[2], angles_intersect[3], num_points + 2)[1:-1]\n",
        "    angles_interpolate3 = np.linspace(angles_intersect[3], angles_intersect[0], num_points + 2)[1:-1]\n",
        "    angles_interpolate4 = np.linspace(angles_intersect[0], angles_intersect[1] + 2 * np.pi, num_points + 2)[1:-1]\n",
        "    angles_total = np.concatenate((angles_interpolate1, angles_interpolate2, angles_interpolate3, angles_interpolate4))\n",
        "    # Points on the perimeter of ellipse\n",
        "    domain_points_perimeter = ellipse(x_cen, y_cen, a, b, gamma, angles_total)\n",
        "    # Points on the sides of ellipse\n",
        "    points_interpolate = np.linspace(0, side_length, num_points + 2)[1:-1]\n",
        "    domain_points_bot = np.column_stack((points_interpolate, np.zeros_like(points_interpolate)))\n",
        "    domain_points_right = np.column_stack((side_length * np.ones_like(points_interpolate), points_interpolate))\n",
        "    domain_points_top = np.column_stack((points_interpolate[::-1], side_length * np.ones_like(points_interpolate)))\n",
        "    domain_points_left = np.column_stack((np.zeros_like(points_interpolate), points_interpolate[::-1]))\n",
        "    domain_points_sides = np.concatenate((domain_points_bot, domain_points_right, domain_points_top, domain_points_left), axis=0)\n",
        "    # Domain points\n",
        "    domain_points_x = np.linspace(domain_points_perimeter[:, 0], domain_points_sides[:, 0], num_points + 2)[1:-1]\n",
        "    domain_points_y = np.linspace(domain_points_perimeter[:, 1], domain_points_sides[:, 1], num_points + 2)[1:-1]\n",
        "    domain_points = np.column_stack((domain_points_x.ravel(), domain_points_y.ravel()))\n",
        "\n",
        "    # Define the loss function\n",
        "    loss_pde1 = pde_residual1(params, domain_points) # parameters to be used in \"model.apply\" should be in the dict with 'params' key.\n",
        "    loss_pde2 = pde_residual2(params, domain_points)\n",
        "    loss_dirichlet1 = dirichlet_residual1(params, boundary_points)\n",
        "    loss_dirichlet2 = dirichlet_residual2(params, boundary_points)\n",
        "    loss_dirichlet3 = dirichlet_residual3(params, boundary_points)\n",
        "    loss_dirichlet4 = dirichlet_residual4(params, boundary_points)\n",
        "    loss_neumann1 = neumann_residual1(params, boundary_points)\n",
        "    loss_neumann2 = neumann_residual2(params, boundary_points)\n",
        "    loss_neumann3 = neumann_residual3(params, void_boundary_points)\n",
        "    \n",
        "    individual_losses = {'pde1': loss_pde1, 'pde2': loss_pde2,\n",
        "                            'dirichlet1': loss_dirichlet1, 'dirichlet2': loss_dirichlet2,\n",
        "                            'dirichlet3': loss_dirichlet3, 'dirichlet4': loss_dirichlet4,\n",
        "                            'neumann1': loss_neumann1, 'neumann2': loss_neumann2, 'neumann3': loss_neumann3}\n",
        "    total_loss = sum(individual_losses.values())\n",
        "\n",
        "    return total_loss, individual_losses\n",
        "\n",
        "# Define Training Step\n",
        "@partial(jax.jit, static_argnums = (1,))\n",
        "def training_step(params, opt, opt_state, key):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        params: network + geometric parameters\n",
        "        opt: optimizer\n",
        "        opt_state: optimizer state\n",
        "        key: random key for sampling\n",
        "    \"\"\"\n",
        "    # Generate random samples (\"jax.grad\" cannot receive the function with randomness)\n",
        "    boundary_samples = lhs(2, 1000)\n",
        "    angle_samples1 = lhs(1, 1000)\n",
        "\n",
        "    # Evaluate the loss function and its gradient\n",
        "    (loss_val, individual_losses), grad = jax.value_and_grad(lambda x: loss_total(x, boundary_samples, angle_samples1), has_aux=True)(params)\n",
        "    \n",
        "    # Update model parameters\n",
        "    update, opt_state = opt.update(grad, opt_state, params) # update using \"grad\"\n",
        "    params = optax.apply_updates(params, update) # apply updates to \"params\"\n",
        "    return params, opt_state, key, loss_val, individual_losses\n",
        "\n",
        "# Training loop\n",
        "def train_loop(params, adam, opt_state, key):\n",
        "    loss_history = {'total': [], 'pde1': [], 'pde2': [],\n",
        "                    'dirichlet1': [], 'dirichlet2': [], 'dirichlet3': [], 'dirichlet4': [],\n",
        "                    'neumann1': [], 'neumann2': [], 'neumann3': []}\n",
        "    for epoch in range(num_epochs): # \"_\" is used because the variable is not used in for loop\n",
        "        params, opt_state, key, loss_val, individual_losses = training_step(params, adam, opt_state, key)\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            # Append the current loss values to their respective lists\n",
        "            loss_history['total'].append(loss_val.item())\n",
        "            for loss_name, loss_value in individual_losses.items():\n",
        "                loss_history[loss_name].append(loss_value.item())\n",
        "    return loss_history, params, opt_state, key, loss_val # return final values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb6bl4IWY4dt"
      },
      "source": [
        "# Helper Functions for L-BFGS Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JRQDgwrzY2JN"
      },
      "outputs": [],
      "source": [
        "# L-BFGS requires the parameters to be a single flattened array!\n",
        "def concat_params(params): # flatten the parameters\n",
        "    params, tree = jax.tree_util.tree_flatten(params) # \"params\" is flattened to a list of arrays\n",
        "    shapes = [param.shape for param in params] # shape of each array in the \"params\" list\n",
        "    return np.concatenate([param.reshape(-1) for param in params]), tree, shapes # concat to single 1D array\n",
        "\n",
        "def unconcat_params(params, tree, shapes): # unflatten the parameters\n",
        "    split_vec = np.split(params, onp.cumsum([onp.prod(shape, dtype=onp.int32) for shape in shapes])) # \"np.cumsum\" figures out the boundaries where to split the flattened \"params\"\n",
        "    split_vec = [vec.reshape(*shape) for vec, shape in zip(split_vec, shapes)] # reshape slices of vector (\"*\" unpack the tuple into individual arguments)\n",
        "    return jax.tree_util.tree_unflatten(tree, split_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7PppXwqY74_"
      },
      "source": [
        "# Evaluation Points & Ground Truth Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS2fRgkoY5q2",
        "outputId": "0d03f408-ef3a-49d0-f56b-89716a606af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation points:  (40401, 2)\n"
          ]
        }
      ],
      "source": [
        "# Create evaluation points\n",
        "x_points = np.linspace(0, 40, 200 + 1)\n",
        "y_points = np.linspace(0, 40, 200 + 1)\n",
        "X, Y = np.meshgrid(x_points, y_points, indexing='ij')\n",
        "X, Y = X.flatten(), Y.flatten()\n",
        "eval_points = np.array([X, Y]).T\n",
        "\n",
        "# Load evaluation solutions (by Ground Truth FEM)\n",
        "with open('data/eval_solutions-dirhchlet10-size_L.json', 'r') as f:\n",
        "    eval_sol = json.load(f)\n",
        "\n",
        "# Ground truth soution (200 x 200 cells)\n",
        "u_true = np.array(eval_sol) # shape: (n_points, 2)\n",
        "\n",
        "print(\"Evaluation points: \", np.array(eval_points).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlORcqS8ZQHY"
      },
      "source": [
        "# Train PINN & Approximate Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A66va-POY_j6",
        "outputId": "00685700-02f3-4108-fc85-8a2182c7f682"
      },
      "outputs": [],
      "source": [
        "# Containers for the results\n",
        "y_results, domain_pts, times_adam, times_lbfgs, times_total, times_eval, l2_rel, arch, losses\\\n",
        "    = dict({}), dict({}), dict({}), dict({}), dict({}), dict({}), dict({}), dict({}), dict({})\n",
        "\n",
        "count = 0 # architecture index\n",
        "for architecture in architecture_list :\n",
        "    print('Architecture : %s' %architecture)\n",
        "    domain_pts_temp = []\n",
        "    # Initialize Model\n",
        "    model = PDESolution(architecture)\n",
        "    key, key2 = jax.random.split(jax.random.PRNGKey(0)) # create two keys for independent use\n",
        "    batch_dim = 8 # dummy number(can be any value). Batch dimension will be reshaped after running \"model.init\".\n",
        "    feature_dim = 2 # dimension of input point (x, y coord)\n",
        "    # \"flax.linen.Module.init(key, dummy_input)\" triggers \"__call__\" (like \".apply\") and infers the shape of weights & biases\n",
        "    # where \"key\" is JAX-made PRNG key and \"dummy_input\" is the dummy input data.\n",
        "    params = model.init(key, np.ones((batch_dim, feature_dim))) # net_params: weights & biases initialized randomly\n",
        "    # geo_params = np.array([25., 30., 5.0, 2.0, np.pi/3]) # geo_params: x, y, a, b, gamma initialized by 1.0\n",
        "\n",
        "    # Initialize Adam Optimizer\n",
        "    adam = optax.adam(learning_rate = lr)\n",
        "    opt_state = adam.init(params) # opt_state : internal states of the Adam optimizer\n",
        "\n",
        "    # Start Training with Adam Optimizer\n",
        "    start_time = time.time()\n",
        "    loss_history, params, opt_state, key, loss_val = jax.block_until_ready(train_loop(params, adam, opt_state, key))\n",
        "    adam_time = time.time() - start_time\n",
        "    print('Adam Training Time : %f secs' %adam_time)\n",
        "\n",
        "    # Generate random samples for L-BFGS optimization\n",
        "    lbfgs_boundary_samples = lhs(2, 1000)\n",
        "    lbfgs_angle_samples1 = lhs(1, 1000)\n",
        "\n",
        "    \n",
        "\n",
        "    init_point, tree, shapes = concat_params(params)\n",
        "\n",
        "    # L-BFGS Optimization\n",
        "    print('Starting L-BFGS Optimization')\n",
        "    start_time2 = time.time()\n",
        "    results = tfp.optimizer.lbfgs_minimize(jax.value_and_grad(lambda x: loss_total(unconcat_params(x, tree, shapes),\n",
        "                                                                                    lbfgs_boundary_samples, lbfgs_angle_samples1)[0]),\n",
        "                                            init_point,\n",
        "                                            max_iterations = 50000,\n",
        "                                            num_correction_pairs = 50, # number of past updates to use for the approximation of the Hessian inverse.\n",
        "                                            f_relative_tolerance = 1.0*np.finfo(float).eps) # stopping criterion\n",
        "    lbfgs_time = time.time() - start_time2\n",
        "    total_time = adam_time + lbfgs_time\n",
        "    print(\"L-BFGS training time : %.3f secs\" % lbfgs_time)\n",
        "\n",
        "    # Comparison to Ground Truth\n",
        "    tuned_params = unconcat_params(results.position, tree, shapes)\n",
        "\n",
        "    start_time3 = time.time()\n",
        "    # Pass the \"eval_points\" to the trained model\n",
        "    u_approx = jax.block_until_ready(model.apply(tuned_params, np.stack((eval_points[:, 0], eval_points[:, 1]), axis=1)))\n",
        "    eval_time = time.time() - start_time3\n",
        "\n",
        "    # MSE\n",
        "    run_accuracy = (onp.linalg.norm(u_approx - u_true)) / onp.linalg.norm(u_true) # relative L2 error\n",
        "\n",
        "    y_gt = u_true.tolist() # for storing into dict\n",
        "    y_results[count] = u_approx.tolist()\n",
        "    domain_pts[count] = eval_points.tolist()\n",
        "    times_adam[count] = adam_time\n",
        "    times_lbfgs[count] = lbfgs_time\n",
        "    times_total[count] = total_time\n",
        "    times_eval[count] = eval_time\n",
        "    l2_rel[count] = onp.mean(run_accuracy).tolist()\n",
        "    arch[count] = architecture_list[count]\n",
        "    losses[count] = loss_history\n",
        "    count += 1\n",
        "\n",
        "\n",
        "    results = dict({'domain_pts': domain_pts,\n",
        "                    'y_results': y_results,\n",
        "                    'y_gt': y_gt})\n",
        "\n",
        "    evaluation = dict({'arch': arch,\n",
        "                       'times_adam': times_adam,\n",
        "                       'times_lbfgs': times_lbfgs,\n",
        "                       'times_total': times_total,\n",
        "                       'times_eval': times_eval,\n",
        "                       'l2_rel': l2_rel,\n",
        "                       'losses_adam': losses})\n",
        "\n",
        "    sol_json = 'PINNs_results.json'\n",
        "    with open(sol_json, \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    eval_json = 'PINNs_evaluation.json'\n",
        "    with open(eval_json, \"w\") as f:\n",
        "        json.dump(evaluation, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LHJvv2zGszk"
      },
      "source": [
        "# Displacement Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VLyjd3GiZUCL",
        "outputId": "2787a52f-09b3-4070-aa05-d766c60a1985"
      },
      "outputs": [],
      "source": [
        "# Create evaluation points\n",
        "x_points = np.linspace(0, 40, 200 + 1)\n",
        "y_points = np.linspace(0, 40, 200 + 1)\n",
        "X, Y = np.meshgrid(x_points, y_points, indexing='ij')\n",
        "X, Y = X.flatten(), Y.flatten()\n",
        "eval_points = np.array([X, Y]).T\n",
        "\n",
        "with open('PINNs_results.json', 'r') as f:\n",
        "    data_results = json.load(f)\n",
        "\n",
        "with open('PINNs_evaluation.json', 'r') as f:\n",
        "    data_eval = json.load(f)\n",
        "\n",
        "# Slice results data\n",
        "domain_pts = data_results['domain_pts']\n",
        "y_results = data_results['y_results']\n",
        "y_gt = data_results['y_gt']\n",
        "\n",
        "arch = data_eval['arch']\n",
        "l2_rel = data_eval['l2_rel']\n",
        "\n",
        "# Exact solution\n",
        "u_exact = np.array(y_gt) # (n,)\n",
        "u_exact_magnitude = np.sqrt(u_exact[:, 0]**2 + u_exact[:, 1]**2)\n",
        "\n",
        "# Contour plot settings\n",
        "# For a consistent scale across all time steps\n",
        "u_min = u_exact_magnitude.min()\n",
        "u_max = u_exact_magnitude.max()\n",
        "# Create n levels between u_min & u_max:\n",
        "num_levels = 80\n",
        "levels = np.linspace(u_min, u_max, num_levels)\n",
        "\n",
        "# Exact solution\n",
        "fig1 = plt.figure(1, figsize=(6, 5))\n",
        "ax1 = fig1.add_subplot(1, 1, 1)\n",
        "sc1 = ax1.tricontourf(X, Y, u_exact_magnitude,\n",
        "                      levels=levels, cmap='viridis')\n",
        "ax1.set_title('Exact Solution')\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_aspect('equal', adjustable='box')\n",
        "fig1.colorbar(sc1, ax=ax1, shrink=0.75, label='Solution Value')\n",
        "\n",
        "# Approximate solution\n",
        "for idx, architecture in arch.items():\n",
        "    # Get the approximate solution for this architecture\n",
        "    u_approx = np.array(y_results[idx])  # (n,)\n",
        "    u_approx_magnitude = np.sqrt(u_approx[:, 0]**2 + u_approx[:, 1]**2)\n",
        "\n",
        "    # Contour plot settings\n",
        "    # For a consistent scale across all time steps\n",
        "    u_min = u_approx_magnitude.min()\n",
        "    u_max = u_approx_magnitude.max()\n",
        "    # Create n levels between u_min & u_max:\n",
        "    num_levels = 80\n",
        "    levels = np.linspace(u_min, u_max, num_levels)\n",
        "\n",
        "    # Contour plot\n",
        "    fig2 = plt.figure(2, figsize=(6, 5))\n",
        "    ax2 = fig2.add_subplot(1, 1, 1)\n",
        "    sc2 = ax2.tricontourf(X, Y, u_approx_magnitude,\n",
        "                          levels=levels, cmap='viridis')\n",
        "    ax2.set_title(f\"PINNs Solution (Architecture={architecture})\")\n",
        "    ax2.set_xlabel(\"x\")\n",
        "    ax2.set_ylabel(\"y\")\n",
        "    ax2.set_aspect('equal', adjustable='box')\n",
        "    fig2.colorbar(sc2, ax=ax2, shrink=0.75, label='Solution Value')\n",
        "\n",
        "    # Save figures\n",
        "    # fig_dir = f'./fig/arch_{idx}'\n",
        "    # if not os.path.exists(fig_dir):\n",
        "    #     os.makedirs(fig_dir, exist_ok=True)\n",
        "\n",
        "    # filename = os.path.join(fig_dir, f'sol_{idx}.png')\n",
        "    # fig2.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close(fig2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8b_f_AJXxma"
      },
      "source": [
        "# Deformed Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "sCss79u7S7ji",
        "outputId": "837ae7b0-6e21-40b8-85e1-325364cb30f6"
      },
      "outputs": [],
      "source": [
        "# Find the boundary indices\n",
        "eval_points = onp.array(eval_points)\n",
        "boundary_idx = onp.where((eval_points[:, 0] == 0) | (eval_points[:, 0] == 40) | (eval_points[:, 1] == 0) | (eval_points[:, 1] == 40))\n",
        "\n",
        "# Arrange approximate displacements\n",
        "u_approx_converted = onp.array(y_results['0'])\n",
        "\n",
        "# Exact solution\n",
        "fig_3 = plt.figure(3, figsize=(6, 5))\n",
        "ax_3 = fig_3.add_subplot(1, 1, 1)\n",
        "ax_3.plot(eval_points[boundary_idx, 0], eval_points[boundary_idx, 1], 'k.', markersize=1, label='Boundary')\n",
        "ax_3.plot(eval_points[boundary_idx, 0] + u_exact[boundary_idx, 0], eval_points[boundary_idx, 1] + u_exact[boundary_idx, 1], 'r.', markersize=2, label='Reference')\n",
        "ax_3.set_title('Deformed Configuration (Exact Solution)')\n",
        "ax_3.set_xlabel('x')\n",
        "ax_3.set_ylabel('y')\n",
        "ax_3.set_aspect('equal', adjustable='box')\n",
        "# fig_3.savefig('./fig/ref_config.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close(fig_3)\n",
        "\n",
        "# Approx solution\n",
        "fig_4 = plt.figure(4, figsize=(6, 5))\n",
        "ax_4 = fig_4.add_subplot(1, 1, 1)\n",
        "ax_4.plot(eval_points[boundary_idx, 0], eval_points[boundary_idx, 1], 'k.', markersize=1, label='Boundary')\n",
        "ax_4.plot(eval_points[boundary_idx, 0] + u_approx_converted[boundary_idx, 0], eval_points[boundary_idx, 1] + u_approx_converted[boundary_idx, 1], 'r.', markersize=2, label='Reference')\n",
        "ax_4.set_title('Deformed Configuration (Approx Solution)')\n",
        "ax_4.set_xlabel('x')\n",
        "ax_4.set_ylabel('y')\n",
        "ax_4.set_aspect('equal', adjustable='box')\n",
        "# fig_3.savefig('./fig/ref_config.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close(fig_4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('PINNs_evaluation.json', 'r') as f:\n",
        "    data_eval = json.load(f)\n",
        "\n",
        "# Slice results data\n",
        "arch = data_eval['arch']\n",
        "l2_rel = data_eval['l2_rel']\n",
        "losses = data_eval['losses_adam']\n",
        "\n",
        "# Loss history of choosen architecture\n",
        "loss_history = losses['0']\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig_loss, ax_loss = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "# Generate X-axis (Epochs)\n",
        "num_data_points = len(loss_history['total'])\n",
        "epochs = [(i + 1) * 100 for i in range(num_data_points)]\n",
        "\n",
        "# Plot Each Loss Term\n",
        "for loss_name, loss_values in loss_history.items():\n",
        "    ax_loss.plot(epochs, loss_values, label=loss_name.capitalize())\n",
        "# ax_loss.plot(epochs, loss_history['dirichlet1'], label=loss_name.capitalize())\n",
        "# ax_loss.plot(epochs, loss_history['dirichlet2'], label=loss_name.capitalize())\n",
        "# ax_loss.plot(epochs, loss_history['dirichlet3'], label=loss_name.capitalize())\n",
        "# ax_loss.plot(epochs, loss_history['dirichlet4'], label=loss_name.capitalize())\n",
        "\n",
        "ax_loss.set_yscale('log') # Use a logarithmic scale\n",
        "ax_loss.legend(loc='lower right', fontsize=10)\n",
        "ax_loss.set_xlabel('Epoch')\n",
        "ax_loss.set_ylabel('Loss (Log Scale)')\n",
        "ax_loss.set_title(f'Loss History')\n",
        "ax_loss.tick_params(axis='both', which='major', labelsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close(fig_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
